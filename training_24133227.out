✅ Conda env active at: /n/fs/similarity/open-r1/openr1/bin/python
Python 3.11.12
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: write).
The token `ionic` has been saved to /n/fs/similarity/open-r1/.hf_cache/stored_tokens
Your token has been saved to /n/fs/similarity/open-r1/.hf_cache/token
Login successful.
Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.
✅ Logged into Hugging Face
Requirement already satisfied: yq in ./openr1/lib/python3.11/site-packages (3.4.3)
Requirement already satisfied: huggingface_hub in ./openr1/lib/python3.11/site-packages (0.32.4)
Requirement already satisfied: PyYAML>=5.3.1 in ./openr1/lib/python3.11/site-packages (from yq) (6.0.2)
Requirement already satisfied: xmltodict>=0.11.0 in ./openr1/lib/python3.11/site-packages (from yq) (0.14.2)
Requirement already satisfied: tomlkit>=0.11.6 in ./openr1/lib/python3.11/site-packages (from yq) (0.13.2)
Requirement already satisfied: argcomplete>=1.8.1 in ./openr1/lib/python3.11/site-packages (from yq) (3.6.2)
Requirement already satisfied: filelock in ./openr1/lib/python3.11/site-packages (from huggingface_hub) (3.18.0)
Requirement already satisfied: fsspec>=2023.5.0 in ./openr1/lib/python3.11/site-packages (from huggingface_hub) (2024.6.1)
Requirement already satisfied: packaging>=20.9 in ./openr1/lib/python3.11/site-packages (from huggingface_hub) (25.0)
Requirement already satisfied: requests in ./openr1/lib/python3.11/site-packages (from huggingface_hub) (2.32.3)
Requirement already satisfied: tqdm>=4.42.1 in ./openr1/lib/python3.11/site-packages (from huggingface_hub) (4.67.1)
Requirement already satisfied: typing-extensions>=3.7.4.3 in ./openr1/lib/python3.11/site-packages (from huggingface_hub) (4.12.2)
Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in ./openr1/lib/python3.11/site-packages (from huggingface_hub) (1.1.2)
Requirement already satisfied: charset-normalizer<4,>=2 in ./openr1/lib/python3.11/site-packages (from requests->huggingface_hub) (3.4.2)
Requirement already satisfied: idna<4,>=2.5 in ./openr1/lib/python3.11/site-packages (from requests->huggingface_hub) (3.10)
Requirement already satisfied: urllib3<3,>=1.21.1 in ./openr1/lib/python3.11/site-packages (from requests->huggingface_hub) (2.4.0)
Requirement already satisfied: certifi>=2017.4.17 in ./openr1/lib/python3.11/site-packages (from requests->huggingface_hub) (2025.4.26)
🟢 Setup complete. Ready to run SFT.
Env:        /n/fs/similarity/open-r1/openr1
Config:     recipes/Qwen2.5-1.5B-Instruct/sft/config_demo_liv.yaml
Log Files:  logs/liv_train_Qwen1.5B-SFT-Finetune-v2_20250606_000856.log
CUDA_VISIBLE_DEVICES: 0,1
/n/fs/similarity/open-r1/openr1/lib/python3.11/site-packages/transformers/utils/hub.py:105: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[2025-06-06 00:09:03,575] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
W0606 00:09:04.892000 1950090 torch/distributed/run.py:792] 
W0606 00:09:04.892000 1950090 torch/distributed/run.py:792] *****************************************
W0606 00:09:04.892000 1950090 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0606 00:09:04.892000 1950090 torch/distributed/run.py:792] *****************************************
/n/fs/similarity/open-r1/openr1/lib/python3.11/site-packages/transformers/utils/hub.py:105: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/n/fs/similarity/open-r1/openr1/lib/python3.11/site-packages/transformers/utils/hub.py:105: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[2025-06-06 00:09:10,243] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-06 00:09:10,244] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /n/fs/similarity/open-r1/.triton, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /n/fs/similarity/open-r1/.triton, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[2025-06-06 00:09:11,197] [INFO] [comm.py:652:init_distributed] cdb=None
[2025-06-06 00:09:11,211] [INFO] [comm.py:652:init_distributed] cdb=None
[2025-06-06 00:09:11,211] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
2025-06-06 00:09:11 - INFO - __main__ - Model parameters ModelConfig(model_name_or_path='Qwen/Qwen2.5-1.5B-Instruct', model_revision='main', torch_dtype='bfloat16', trust_remote_code=False, attn_implementation='flash_attention_2', use_peft=False, lora_r=16, lora_alpha=32, lora_dropout=0.05, lora_target_modules=None, lora_modules_to_save=None, lora_task_type='CAUSAL_LM', use_rslora=False, use_dora=False, load_in_8bit=False, load_in_4bit=False, bnb_4bit_quant_type='nf4', use_bnb_nested_quant=False)
2025-06-06 00:09:11 - INFO - __main__ - Script parameters ScriptArguments(dataset_name='open-r1/OpenR1-Math-220k', dataset_config='default', dataset_train_split='train', dataset_test_split='test', gradient_checkpointing_use_reentrant=False, ignore_bias_buffers=False, dataset_prompt_column='problem', dataset_response_column='solution', dataset_mixture=None)
2025-06-06 00:09:11 - INFO - __main__ - Training parameters SFTConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
benchmarks=[],
bf16=True,
bf16_full_eval=False,
callbacks=[],
chars_per_token=<CHARS_PER_TOKEN>,
chat_template=None,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
dataset_batch_size=None,
dataset_kwargs=None,
dataset_num_proc=None,
dataset_text_field=text,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=4,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_packing=None,
eval_steps=500000,
eval_strategy=IntervalStrategy.STEPS,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs={'use_reentrant': False},
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=Qwen2.5-1.5B-Instruct-SFT,
hub_model_revision=main,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-06,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=info,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/n/fs/similarity/open-r1/data/Qwen2.5-1.5B-Instruct-SFT-v4/runs/Jun06_00-09-11_node207.ionic.cs.princeton.edu,
logging_first_step=True,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_length=1024,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
model_init_kwargs=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_of_sequences=None,
num_train_epochs=1,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=/n/fs/similarity/open-r1/data/Qwen2.5-1.5B-Instruct-SFT-v4,
overwrite_hub_revision=False,
overwrite_output_dir=True,
packing=False,
padding_free=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=2,
prediction_loss_only=False,
push_to_hub=True,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_revision=False,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=Qwen1.5B-SFT-Finetune-v2-20250606_000856,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=50,
save_strategy=SaveStrategy.STEPS,
save_total_limit=100,
seed=42,
skip_memory_metrics=True,
split_batches=None,
system_prompt=You are a helpful AI Assistant that provides well-reasoned and detailed responses. You first think about the reasoning process as an internal monologue and then provide the user with the answer. Respond in the following format:
<think> ... </think> <answer> ... </answer>
,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tp_size=0,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger=None,
use_liger_kernel=False,
use_mps_device=False,
wandb_entity=None,
wandb_project=None,
wandb_run_group=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=0.0,
)
2025-06-06 00:09:11 - INFO - open_r1.utils.data - Loading dataset: open-r1/OpenR1-Math-220k/default
2025-06-06 00:09:11 - INFO - open_r1.utils.data - Loading dataset: open-r1/OpenR1-Math-220k/default
2025-06-06 00:09:13 - INFO - open_r1.utils.data - Tokenizing split 'train' (93733 examples)…
2025-06-06 00:09:13 - WARNING - __main__ - Requested split 'test' not found in ['train']. Creating a 1 % evaluation split from the training data.
[2025-06-06 00:09:13,556] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2
[WARNING|logging.py:329] 2025-06-06 00:09:13,558 >> You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Overwrite dataset info from restored data version if exists.
2025-06-06 00:09:13 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /n/fs/similarity/open-r1/.cache/huggingface/datasets/open-r1___open_r1-math-220k/default/0.0.0/e4e141ec9dea9f8326f4d347be56105859b2bd68
2025-06-06 00:09:13 - INFO - datasets.info - Loading Dataset info from /n/fs/similarity/open-r1/.cache/huggingface/datasets/open-r1___open_r1-math-220k/default/0.0.0/e4e141ec9dea9f8326f4d347be56105859b2bd68
Found cached dataset open_r1-math-220k (/n/fs/similarity/open-r1/.cache/huggingface/datasets/open-r1___open_r1-math-220k/default/0.0.0/e4e141ec9dea9f8326f4d347be56105859b2bd68)
2025-06-06 00:09:13 - INFO - datasets.builder - Found cached dataset open_r1-math-220k (/n/fs/similarity/open-r1/.cache/huggingface/datasets/open-r1___open_r1-math-220k/default/0.0.0/e4e141ec9dea9f8326f4d347be56105859b2bd68)
Loading Dataset info from /n/fs/similarity/open-r1/.cache/huggingface/datasets/open-r1___open_r1-math-220k/default/0.0.0/e4e141ec9dea9f8326f4d347be56105859b2bd68
2025-06-06 00:09:13 - INFO - datasets.info - Loading Dataset info from /n/fs/similarity/open-r1/.cache/huggingface/datasets/open-r1___open_r1-math-220k/default/0.0.0/e4e141ec9dea9f8326f4d347be56105859b2bd68
[INFO|tokenization_utils_base.py:2060] 2025-06-06 00:09:14,001 >> loading file vocab.json from cache at /n/fs/similarity/open-r1/.cache/huggingface/transformers/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/vocab.json
[INFO|tokenization_utils_base.py:2060] 2025-06-06 00:09:14,001 >> loading file merges.txt from cache at /n/fs/similarity/open-r1/.cache/huggingface/transformers/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/merges.txt
[INFO|tokenization_utils_base.py:2060] 2025-06-06 00:09:14,001 >> loading file tokenizer.json from cache at /n/fs/similarity/open-r1/.cache/huggingface/transformers/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/tokenizer.json
[INFO|tokenization_utils_base.py:2060] 2025-06-06 00:09:14,001 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2060] 2025-06-06 00:09:14,001 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2060] 2025-06-06 00:09:14,001 >> loading file tokenizer_config.json from cache at /n/fs/similarity/open-r1/.cache/huggingface/transformers/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/tokenizer_config.json
[INFO|tokenization_utils_base.py:2060] 2025-06-06 00:09:14,001 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2323] 2025-06-06 00:09:14,219 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2025-06-06 00:09:14 - INFO - open_r1.utils.data - Tokenizing split 'train' (93733 examples)…
Loading cached processed dataset at /n/fs/similarity/open-r1/.cache/huggingface/datasets/open-r1___open_r1-math-220k/default/0.0.0/e4e141ec9dea9f8326f4d347be56105859b2bd68/cache-6aeed7f1e1542bea.arrow
2025-06-06 00:09:14 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /n/fs/similarity/open-r1/.cache/huggingface/datasets/open-r1___open_r1-math-220k/default/0.0.0/e4e141ec9dea9f8326f4d347be56105859b2bd68/cache-6aeed7f1e1542bea.arrow
2025-06-06 00:09:14 - WARNING - __main__ - Requested split 'test' not found in ['train']. Creating a 1 % evaluation split from the training data.
Loading cached split indices for dataset at /n/fs/similarity/open-r1/.cache/huggingface/datasets/open-r1___open_r1-math-220k/default/0.0.0/e4e141ec9dea9f8326f4d347be56105859b2bd68/cache-f214169554aea0b8.arrow and /n/fs/similarity/open-r1/.cache/huggingface/datasets/open-r1___open_r1-math-220k/default/0.0.0/e4e141ec9dea9f8326f4d347be56105859b2bd68/cache-8ca86c422b54feb8.arrow
2025-06-06 00:09:14 - INFO - datasets.arrow_dataset - Loading cached split indices for dataset at /n/fs/similarity/open-r1/.cache/huggingface/datasets/open-r1___open_r1-math-220k/default/0.0.0/e4e141ec9dea9f8326f4d347be56105859b2bd68/cache-f214169554aea0b8.arrow and /n/fs/similarity/open-r1/.cache/huggingface/datasets/open-r1___open_r1-math-220k/default/0.0.0/e4e141ec9dea9f8326f4d347be56105859b2bd68/cache-8ca86c422b54feb8.arrow
[INFO|tokenization_utils_base.py:2060] 2025-06-06 00:09:14,335 >> loading file vocab.json from cache at /n/fs/similarity/open-r1/.cache/huggingface/transformers/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/vocab.json
[INFO|tokenization_utils_base.py:2060] 2025-06-06 00:09:14,335 >> loading file merges.txt from cache at /n/fs/similarity/open-r1/.cache/huggingface/transformers/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/merges.txt
[INFO|tokenization_utils_base.py:2060] 2025-06-06 00:09:14,335 >> loading file tokenizer.json from cache at /n/fs/similarity/open-r1/.cache/huggingface/transformers/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/tokenizer.json
[INFO|tokenization_utils_base.py:2060] 2025-06-06 00:09:14,335 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2060] 2025-06-06 00:09:14,335 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2060] 2025-06-06 00:09:14,335 >> loading file tokenizer_config.json from cache at /n/fs/similarity/open-r1/.cache/huggingface/transformers/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/tokenizer_config.json
[INFO|tokenization_utils_base.py:2060] 2025-06-06 00:09:14,335 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2323] 2025-06-06 00:09:14,543 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:699] 2025-06-06 00:09:14,588 >> loading configuration file config.json from cache at /n/fs/similarity/open-r1/.cache/huggingface/transformers/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json
[INFO|configuration_utils.py:771] 2025-06-06 00:09:14,590 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1536,
  "initializer_range": 0.02,
  "intermediate_size": 8960,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 12,
  "num_hidden_layers": 28,
  "num_key_value_heads": 2,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.50.0",
  "use_cache": false,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|modeling_utils.py:1154] 2025-06-06 00:09:14,638 >> loading weights file model.safetensors from cache at /n/fs/similarity/open-r1/.cache/huggingface/transformers/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/model.safetensors
[INFO|modeling_utils.py:2170] 2025-06-06 00:09:14,639 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.
[INFO|modeling_utils.py:3747] 2025-06-06 00:09:14,639 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[2025-06-06 00:09:14,639] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2
[WARNING|logging.py:329] 2025-06-06 00:09:14,642 >> You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[INFO|configuration_utils.py:1139] 2025-06-06 00:09:14,646 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "use_cache": false
}

[2025-06-06 00:09:15,767] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 339, num_elems = 1.78B
[INFO|modeling_utils.py:4987] 2025-06-06 00:09:17,094 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.

[INFO|modeling_utils.py:4995] 2025-06-06 00:09:17,094 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2.5-1.5B-Instruct.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1094] 2025-06-06 00:09:17,135 >> loading configuration file generation_config.json from cache at /n/fs/similarity/open-r1/.cache/huggingface/transformers/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/generation_config.json
[INFO|configuration_utils.py:1139] 2025-06-06 00:09:17,136 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.1,
  "temperature": 0.7,
  "top_k": 20,
  "top_p": 0.8
}

Loading cached processed dataset at /n/fs/similarity/open-r1/.cache/huggingface/datasets/open-r1___open_r1-math-220k/default/0.0.0/e4e141ec9dea9f8326f4d347be56105859b2bd68/cache-212db3323b9eb092.arrow
2025-06-06 00:09:17 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /n/fs/similarity/open-r1/.cache/huggingface/datasets/open-r1___open_r1-math-220k/default/0.0.0/e4e141ec9dea9f8326f4d347be56105859b2bd68/cache-212db3323b9eb092.arrow
Loading cached processed dataset at /n/fs/similarity/open-r1/.cache/huggingface/datasets/open-r1___open_r1-math-220k/default/0.0.0/e4e141ec9dea9f8326f4d347be56105859b2bd68/cache-eb0e30a8f560726f.arrow
2025-06-06 00:09:17 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /n/fs/similarity/open-r1/.cache/huggingface/datasets/open-r1___open_r1-math-220k/default/0.0.0/e4e141ec9dea9f8326f4d347be56105859b2bd68/cache-eb0e30a8f560726f.arrow
[INFO|trainer.py:748] 2025-06-06 00:09:17,197 >> Using auto half precision backend
2025-06-06 00:09:17 - INFO - __main__ - *** Train ***
[INFO|deepspeed.py:386] 2025-06-06 00:09:17,433 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
Using /n/fs/similarity/open-r1/.cache/torch_extensions/py311_cu124 as PyTorch extensions root...
Emitting ninja build file /n/fs/similarity/open-r1/.cache/torch_extensions/py311_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Using /n/fs/similarity/open-r1/.cache/torch_extensions/py311_cu124 as PyTorch extensions root...
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 0.2819085121154785 seconds
Loading extension module cpu_adam...
Time to load cpu_adam op: 0.3584756851196289 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000002, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
[2025-06-06 00:09:18,033] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.15.4, git-hash=unknown, git-branch=unknown
[2025-06-06 00:09:18,033] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2
[2025-06-06 00:09:18,043] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-06-06 00:09:18,044] [INFO] [logging.py:128:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2025-06-06 00:09:18,044] [INFO] [logging.py:128:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-06-06 00:09:18,059] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2025-06-06 00:09:18,059] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2025-06-06 00:09:18,059] [INFO] [logging.py:128:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
[2025-06-06 00:09:18,060] [INFO] [logging.py:128:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
[2025-06-06 00:09:18,241] [INFO] [utils.py:781:see_memory_usage] Stage 3 initialize beginning
[2025-06-06 00:09:18,242] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.87 GB         CA 0.0 GB         Max_CA 1 GB 
[2025-06-06 00:09:18,242] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 23.48 GB, percent = 4.7%
[2025-06-06 00:09:18,243] [INFO] [stage3.py:166:__init__] Reduce bucket size 500000000
[2025-06-06 00:09:18,243] [INFO] [stage3.py:167:__init__] Prefetch bucket size 50000000
[2025-06-06 00:09:18,389] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2025-06-06 00:09:18,390] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
[2025-06-06 00:09:18,390] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 23.48 GB, percent = 4.7%
Parameter Offload: Total persistent parameters: 144896 in 141 params
[2025-06-06 00:09:18,549] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2025-06-06 00:09:18,550] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
[2025-06-06 00:09:18,550] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 23.48 GB, percent = 4.7%
[2025-06-06 00:09:18,700] [INFO] [utils.py:781:see_memory_usage] Before creating fp16 partitions
[2025-06-06 00:09:18,700] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
[2025-06-06 00:09:18,700] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 23.48 GB, percent = 4.7%
[2025-06-06 00:09:19,937] [INFO] [utils.py:781:see_memory_usage] After creating fp16 partitions: 2
[2025-06-06 00:09:19,938] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
[2025-06-06 00:09:19,938] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 28.1 GB, percent = 5.6%
[2025-06-06 00:09:20,124] [INFO] [utils.py:781:see_memory_usage] Before creating fp32 partitions
[2025-06-06 00:09:20,125] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
[2025-06-06 00:09:20,125] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 28.87 GB, percent = 5.7%
[2025-06-06 00:09:21,243] [INFO] [utils.py:781:see_memory_usage] After creating fp32 partitions
[2025-06-06 00:09:21,244] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
[2025-06-06 00:09:21,244] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 33.96 GB, percent = 6.8%
[2025-06-06 00:09:21,425] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-06-06 00:09:21,425] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
[2025-06-06 00:09:21,425] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 34.41 GB, percent = 6.8%
[2025-06-06 00:09:22,634] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-06-06 00:09:22,634] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
[2025-06-06 00:09:22,634] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 39.8 GB, percent = 7.9%
[2025-06-06 00:09:22,635] [INFO] [stage3.py:521:_setup_for_real_optimizer] optimizer state initialized
[2025-06-06 00:09:23,553] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-06-06 00:09:23,553] [INFO] [utils.py:782:see_memory_usage] MA 0.93 GB         Max_MA 1.8 GB         CA 1.82 GB         Max_CA 2 GB 
[2025-06-06 00:09:23,553] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 42.55 GB, percent = 8.5%
[2025-06-06 00:09:23,554] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer_Stage3
[2025-06-06 00:09:23,554] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2025-06-06 00:09:23,554] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-06-06 00:09:23,554] [INFO] [logging.py:128:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.999), (0.9, 0.999)]
[2025-06-06 00:09:23,554] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   amp_enabled .................. False
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   amp_params ................... False
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f02bbfc4050>
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   communication_data_type ...... None
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   disable_allgather ............ False
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   dump_state ................... False
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   global_rank .................. 0
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 16
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   optimizer_name ............... None
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   optimizer_params ............. None
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   pld_enabled .................. False
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   pld_params ................... False
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   scheduler_name ............... None
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   scheduler_params ............. None
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   sparse_attention ............. None
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   steps_per_print .............. inf
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   train_batch_size ............. 64
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  2
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   world_size ................... 2
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  True
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   zero_enabled ................. True
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3
[2025-06-06 00:09:23,556] [INFO] [config.py:989:print_user_config]   json = {
    "train_batch_size": 64, 
    "train_micro_batch_size_per_gpu": 2, 
    "gradient_accumulation_steps": 16, 
    "zero_optimization": {
        "stage": 3, 
        "offload_optimizer": {
            "device": "cpu", 
            "nvme_path": null
        }, 
        "offload_param": {
            "device": "cpu", 
            "nvme_path": null
        }, 
        "stage3_gather_16bit_weights_on_model_save": true
    }, 
    "gradient_clipping": 1.0, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
[INFO|trainer.py:2409] 2025-06-06 00:09:23,558 >> ***** Running training *****
[INFO|trainer.py:2410] 2025-06-06 00:09:23,558 >>   Num examples = 92,795
[INFO|trainer.py:2411] 2025-06-06 00:09:23,558 >>   Num Epochs = 1
[INFO|trainer.py:2412] 2025-06-06 00:09:23,558 >>   Instantaneous batch size per device = 2
[INFO|trainer.py:2415] 2025-06-06 00:09:23,558 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:2416] 2025-06-06 00:09:23,558 >>   Gradient Accumulation steps = 16
[INFO|trainer.py:2417] 2025-06-06 00:09:23,558 >>   Total optimization steps = 1,449
[INFO|trainer.py:2418] 2025-06-06 00:09:23,559 >>   Number of trainable parameters = 1,543,714,304
[INFO|integration_utils.py:831] 2025-06-06 00:09:23,560 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: Currently logged in as: ogd3 (ogd3-princeton-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /n/fs/similarity/open-r1/.wandb/wandb/run-20250606_000923-v96u0y67
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Qwen1.5B-SFT-Finetune-v2-20250606_000856
wandb: ⭐️ View project at https://wandb.ai/ogd3-princeton-university/huggingface
wandb: 🚀 View run at https://wandb.ai/ogd3-princeton-university/huggingface/runs/v96u0y67
  0%|          | 0/1449 [00:00<?, ?it/s]  0%|          | 1/1449 [00:25<10:17:21, 25.58s/it]                                                   {'loss': 1.4013, 'grad_norm': 7.115165710449219, 'learning_rate': 2.73972602739726e-08, 'num_tokens': 5427.0, 'mean_token_accuracy': 0.6334721110761166, 'epoch': 0.0}
  0%|          | 1/1449 [00:25<10:17:21, 25.58s/it]  0%|          | 2/1449 [00:46<9:17:44, 23.13s/it]                                                   {'loss': 1.2827, 'grad_norm': 6.971931457519531, 'learning_rate': 5.47945205479452e-08, 'num_tokens': 11489.0, 'mean_token_accuracy': 0.6730691567063332, 'epoch': 0.0}
  0%|          | 2/1449 [00:46<9:17:44, 23.13s/it]  0%|          | 3/1449 [01:08<8:54:19, 22.17s/it]                                                  {'loss': 1.193, 'grad_norm': 6.016913414001465, 'learning_rate': 8.21917808219178e-08, 'num_tokens': 18093.0, 'mean_token_accuracy': 0.683920793235302, 'epoch': 0.0}
  0%|          | 3/1449 [01:08<8:54:19, 22.17s/it]  0%|          | 4/1449 [01:29<8:44:12, 21.77s/it]                                                  {'loss': 1.3166, 'grad_norm': 6.0954179763793945, 'learning_rate': 1.095890410958904e-07, 'num_tokens': 25298.0, 'mean_token_accuracy': 0.6579648293554783, 'epoch': 0.0}
  0%|          | 4/1449 [01:29<8:44:12, 21.77s/it]  0%|          | 5/1449 [01:49<8:35:32, 21.42s/it]                                                  {'loss': 1.2731, 'grad_norm': 7.529391288757324, 'learning_rate': 1.36986301369863e-07, 'num_tokens': 30708.0, 'mean_token_accuracy': 0.6679505221545696, 'epoch': 0.0}
  0%|          | 5/1449 [01:49<8:35:32, 21.42s/it]  0%|          | 6/1449 [02:11<8:33:21, 21.35s/it]                                                  {'loss': 1.3951, 'grad_norm': 7.666150093078613, 'learning_rate': 1.643835616438356e-07, 'num_tokens': 36237.0, 'mean_token_accuracy': 0.6484273560345173, 'epoch': 0.0}
  0%|          | 6/1449 [02:11<8:33:21, 21.35s/it]  0%|          | 7/1449 [02:32<8:32:03, 21.31s/it]                                                  {'loss': 1.3743, 'grad_norm': 6.7357258796691895, 'learning_rate': 1.917808219178082e-07, 'num_tokens': 42724.0, 'mean_token_accuracy': 0.635077141225338, 'epoch': 0.0}
  0%|          | 7/1449 [02:32<8:32:03, 21.31s/it]  1%|          | 8/1449 [02:53<8:28:15, 21.16s/it]                                                  {'loss': 1.3128, 'grad_norm': 7.435177326202393, 'learning_rate': 2.191780821917808e-07, 'num_tokens': 48140.0, 'mean_token_accuracy': 0.6607662849128246, 'epoch': 0.01}
  1%|          | 8/1449 [02:53<8:28:15, 21.16s/it]  1%|          | 9/1449 [03:14<8:30:15, 21.26s/it]                                                  {'loss': 1.2614, 'grad_norm': 6.350414276123047, 'learning_rate': 2.465753424657534e-07, 'num_tokens': 54288.0, 'mean_token_accuracy': 0.676834762096405, 'epoch': 0.01}
  1%|          | 9/1449 [03:14<8:30:15, 21.26s/it]  1%|          | 10/1449 [03:35<8:26:45, 21.13s/it]                                                   {'loss': 1.3057, 'grad_norm': 6.446128845214844, 'learning_rate': 2.73972602739726e-07, 'num_tokens': 60667.0, 'mean_token_accuracy': 0.6524682082235813, 'epoch': 0.01}
  1%|          | 10/1449 [03:35<8:26:45, 21.13s/it]  1%|          | 11/1449 [03:57<8:28:55, 21.23s/it]                                                   {'loss': 1.3567, 'grad_norm': 6.691532135009766, 'learning_rate': 3.013698630136986e-07, 'num_tokens': 66607.0, 'mean_token_accuracy': 0.6365336775779724, 'epoch': 0.01}
  1%|          | 11/1449 [03:57<8:28:55, 21.23s/it]  1%|          | 12/1449 [04:17<8:25:47, 21.12s/it]                                                   {'loss': 1.2435, 'grad_norm': 6.698115825653076, 'learning_rate': 3.287671232876712e-07, 'num_tokens': 72876.0, 'mean_token_accuracy': 0.6751282997429371, 'epoch': 0.01}
  1%|          | 12/1449 [04:17<8:25:47, 21.12s/it]  1%|          | 13/1449 [04:39<8:26:35, 21.17s/it]                                                   {'loss': 1.2595, 'grad_norm': 6.763347625732422, 'learning_rate': 3.561643835616438e-07, 'num_tokens': 78347.0, 'mean_token_accuracy': 0.6725441217422485, 'epoch': 0.01}
  1%|          | 13/1449 [04:39<8:26:35, 21.17s/it]  1%|          | 14/1449 [05:00<8:24:03, 21.08s/it]                                                   {'loss': 1.394, 'grad_norm': 7.906730651855469, 'learning_rate': 3.835616438356164e-07, 'num_tokens': 83513.0, 'mean_token_accuracy': 0.636437427252531, 'epoch': 0.01}
  1%|          | 14/1449 [05:00<8:24:03, 21.08s/it]  1%|          | 15/1449 [05:21<8:23:42, 21.08s/it]                                                   {'loss': 1.1872, 'grad_norm': 6.908790588378906, 'learning_rate': 4.10958904109589e-07, 'num_tokens': 89878.0, 'mean_token_accuracy': 0.6748992577195168, 'epoch': 0.01}
  1%|          | 15/1449 [05:21<8:23:42, 21.08s/it]  1%|          | 16/1449 [05:42<8:23:11, 21.07s/it]                                                   {'loss': 1.4481, 'grad_norm': 6.274990558624268, 'learning_rate': 4.383561643835616e-07, 'num_tokens': 95932.0, 'mean_token_accuracy': 0.6279730424284935, 'epoch': 0.01}
  1%|          | 16/1449 [05:42<8:23:11, 21.07s/it]  1%|          | 17/1449 [06:03<8:22:04, 21.04s/it]                                                   {'loss': 1.3155, 'grad_norm': 7.335964679718018, 'learning_rate': 4.657534246575342e-07, 'num_tokens': 101319.0, 'mean_token_accuracy': 0.6580870747566223, 'epoch': 0.01}
  1%|          | 17/1449 [06:03<8:22:04, 21.04s/it]  1%|          | 18/1449 [06:24<8:24:00, 21.13s/it]                                                   {'loss': 1.3389, 'grad_norm': 6.919323444366455, 'learning_rate': 4.931506849315068e-07, 'num_tokens': 106801.0, 'mean_token_accuracy': 0.6538033038377762, 'epoch': 0.01}
  1%|          | 18/1449 [06:24<8:24:00, 21.13s/it]  1%|▏         | 19/1449 [06:45<8:21:29, 21.04s/it]                                                   {'loss': 1.3946, 'grad_norm': 7.086327075958252, 'learning_rate': 5.205479452054794e-07, 'num_tokens': 112855.0, 'mean_token_accuracy': 0.6417123675346375, 'epoch': 0.01}
  1%|▏         | 19/1449 [06:45<8:21:29, 21.04s/it]  1%|▏         | 20/1449 [07:06<8:22:11, 21.09s/it]                                                   {'loss': 1.2988, 'grad_norm': 7.834427356719971, 'learning_rate': 5.47945205479452e-07, 'num_tokens': 118972.0, 'mean_token_accuracy': 0.6457272693514824, 'epoch': 0.01}
  1%|▏         | 20/1449 [07:06<8:22:11, 21.09s/it]  1%|▏         | 21/1449 [07:27<8:21:18, 21.06s/it]                                                   {'loss': 1.3779, 'grad_norm': 7.654954433441162, 'learning_rate': 5.753424657534246e-07, 'num_tokens': 124312.0, 'mean_token_accuracy': 0.6418646015226841, 'epoch': 0.01}
  1%|▏         | 21/1449 [07:27<8:21:18, 21.06s/it]  2%|▏         | 22/1449 [07:48<8:22:02, 21.11s/it]                                                   {'loss': 1.2275, 'grad_norm': 6.827056884765625, 'learning_rate': 6.027397260273972e-07, 'num_tokens': 130455.0, 'mean_token_accuracy': 0.6807327605783939, 'epoch': 0.02}
  2%|▏         | 22/1449 [07:48<8:22:02, 21.11s/it]  2%|▏         | 23/1449 [08:09<8:19:48, 21.03s/it]                                                   {'loss': 1.4631, 'grad_norm': 7.265579700469971, 'learning_rate': 6.301369863013698e-07, 'num_tokens': 135953.0, 'mean_token_accuracy': 0.6411265209317207, 'epoch': 0.02}
  2%|▏         | 23/1449 [08:09<8:19:48, 21.03s/it]  2%|▏         | 24/1449 [08:30<8:19:59, 21.05s/it]                                                   {'loss': 1.2879, 'grad_norm': 7.849670886993408, 'learning_rate': 6.575342465753423e-07, 'num_tokens': 141579.0, 'mean_token_accuracy': 0.6572689227759838, 'epoch': 0.02}
  2%|▏         | 24/1449 [08:30<8:19:59, 21.05s/it]  2%|▏         | 25/1449 [08:51<8:20:23, 21.08s/it]                                                   {'loss': 1.4533, 'grad_norm': 8.494632720947266, 'learning_rate': 6.84931506849315e-07, 'num_tokens': 146504.0, 'mean_token_accuracy': 0.6254400610923767, 'epoch': 0.02}
  2%|▏         | 25/1449 [08:51<8:20:23, 21.08s/it]  2%|▏         | 26/1449 [09:12<8:18:26, 21.02s/it]                                                   {'loss': 1.3099, 'grad_norm': 6.963324546813965, 'learning_rate': 7.123287671232876e-07, 'num_tokens': 152679.0, 'mean_token_accuracy': 0.6663438230752945, 'epoch': 0.02}
  2%|▏         | 26/1449 [09:12<8:18:26, 21.02s/it]  2%|▏         | 27/1449 [09:33<8:19:20, 21.07s/it]                                                   {'loss': 1.2495, 'grad_norm': 6.746429920196533, 'learning_rate': 7.397260273972602e-07, 'num_tokens': 158329.0, 'mean_token_accuracy': 0.6618391908705235, 'epoch': 0.02}
  2%|▏         | 27/1449 [09:33<8:19:20, 21.07s/it]  2%|▏         | 28/1449 [09:54<8:17:28, 21.01s/it]                                                   {'loss': 1.3328, 'grad_norm': 6.986436367034912, 'learning_rate': 7.671232876712328e-07, 'num_tokens': 163705.0, 'mean_token_accuracy': 0.6490888185799122, 'epoch': 0.02}
  2%|▏         | 28/1449 [09:54<8:17:28, 21.01s/it]  2%|▏         | 29/1449 [10:15<8:18:27, 21.06s/it]                                                   {'loss': 1.3491, 'grad_norm': 6.973186492919922, 'learning_rate': 7.945205479452054e-07, 'num_tokens': 169523.0, 'mean_token_accuracy': 0.6537458896636963, 'epoch': 0.02}
  2%|▏         | 29/1449 [10:15<8:18:27, 21.06s/it]  2%|▏         | 30/1449 [10:36<8:16:40, 21.00s/it]                                                   {'loss': 1.347, 'grad_norm': 6.420229434967041, 'learning_rate': 8.21917808219178e-07, 'num_tokens': 175967.0, 'mean_token_accuracy': 0.648872122168541, 'epoch': 0.02}
  2%|▏         | 30/1449 [10:36<8:16:40, 21.00s/it]  2%|▏         | 31/1449 [10:57<8:17:39, 21.06s/it]                                                   {'loss': 1.3448, 'grad_norm': 6.6433820724487305, 'learning_rate': 8.493150684931506e-07, 'num_tokens': 181689.0, 'mean_token_accuracy': 0.6523785591125488, 'epoch': 0.02}
  2%|▏         | 31/1449 [10:57<8:17:39, 21.06s/it]  2%|▏         | 32/1449 [11:18<8:15:52, 21.00s/it]                                                   {'loss': 1.35, 'grad_norm': 6.745405197143555, 'learning_rate': 8.767123287671232e-07, 'num_tokens': 187044.0, 'mean_token_accuracy': 0.6437696814537048, 'epoch': 0.02}
  2%|▏         | 32/1449 [11:18<8:15:52, 21.00s/it]  2%|▏         | 33/1449 [11:39<8:15:38, 21.00s/it]                                                   {'loss': 1.2935, 'grad_norm': 5.675817489624023, 'learning_rate': 9.041095890410958e-07, 'num_tokens': 192590.0, 'mean_token_accuracy': 0.6676803044974804, 'epoch': 0.02}
  2%|▏         | 33/1449 [11:39<8:15:38, 21.00s/it]  2%|▏         | 34/1449 [12:00<8:15:21, 21.00s/it]                                                   {'loss': 1.203, 'grad_norm': 5.573630332946777, 'learning_rate': 9.315068493150684e-07, 'num_tokens': 198624.0, 'mean_token_accuracy': 0.6784413792192936, 'epoch': 0.02}
  2%|▏         | 34/1449 [12:00<8:15:21, 21.00s/it]  2%|▏         | 35/1449 [12:21<8:14:00, 20.96s/it]                                                   {'loss': 1.33, 'grad_norm': 5.974151611328125, 'learning_rate': 9.58904109589041e-07, 'num_tokens': 204768.0, 'mean_token_accuracy': 0.666594598442316, 'epoch': 0.02}
  2%|▏         | 35/1449 [12:21<8:14:00, 20.96s/it]  2%|▏         | 36/1449 [12:42<8:15:44, 21.05s/it]                                                   {'loss': 1.297, 'grad_norm': 6.121656894683838, 'learning_rate': 9.863013698630137e-07, 'num_tokens': 210397.0, 'mean_token_accuracy': 0.6671470813453197, 'epoch': 0.02}
  2%|▏         | 36/1449 [12:42<8:15:44, 21.05s/it]  3%|▎         | 37/1449 [13:03<8:13:55, 20.99s/it]                                                   {'loss': 1.1921, 'grad_norm': 5.902688980102539, 'learning_rate': 1.0136986301369862e-06, 'num_tokens': 215983.0, 'mean_token_accuracy': 0.6744697690010071, 'epoch': 0.03}
  3%|▎         | 37/1449 [13:03<8:13:55, 20.99s/it]  3%|▎         | 38/1449 [13:25<8:15:22, 21.07s/it]                                                   {'loss': 1.3098, 'grad_norm': 6.261509418487549, 'learning_rate': 1.0410958904109588e-06, 'num_tokens': 221629.0, 'mean_token_accuracy': 0.6626531109213829, 'epoch': 0.03}
  3%|▎         | 38/1449 [13:25<8:15:22, 21.07s/it]  3%|▎         | 39/1449 [13:45<8:13:30, 21.00s/it]                                                   {'loss': 1.4288, 'grad_norm': 6.905079364776611, 'learning_rate': 1.0684931506849315e-06, 'num_tokens': 226507.0, 'mean_token_accuracy': 0.6334438137710094, 'epoch': 0.03}
  3%|▎         | 39/1449 [13:45<8:13:30, 21.00s/it]  3%|▎         | 40/1449 [14:07<8:14:36, 21.06s/it]                                                   {'loss': 1.2419, 'grad_norm': 5.8247880935668945, 'learning_rate': 1.095890410958904e-06, 'num_tokens': 232359.0, 'mean_token_accuracy': 0.6756292916834354, 'epoch': 0.03}
  3%|▎         | 40/1449 [14:07<8:14:36, 21.06s/it]  3%|▎         | 41/1449 [14:27<8:12:41, 21.00s/it]                                                   {'loss': 1.1499, 'grad_norm': 4.8111066818237305, 'learning_rate': 1.1232876712328766e-06, 'num_tokens': 238685.0, 'mean_token_accuracy': 0.6903329975903034, 'epoch': 0.03}
  3%|▎         | 41/1449 [14:27<8:12:41, 21.00s/it]  3%|▎         | 42/1449 [14:49<8:12:36, 21.01s/it]                                                   {'loss': 1.2126, 'grad_norm': 5.329721927642822, 'learning_rate': 1.1506849315068492e-06, 'num_tokens': 244528.0, 'mean_token_accuracy': 0.6731867976486683, 'epoch': 0.03}
  3%|▎         | 42/1449 [14:49<8:12:36, 21.01s/it]  3%|▎         | 43/1449 [15:10<8:12:16, 21.01s/it]                                                   {'loss': 1.178, 'grad_norm': 5.577686786651611, 'learning_rate': 1.178082191780822e-06, 'num_tokens': 250356.0, 'mean_token_accuracy': 0.6719384267926216, 'epoch': 0.03}
  3%|▎         | 43/1449 [15:10<8:12:16, 21.01s/it]  3%|▎         | 44/1449 [15:30<8:10:59, 20.97s/it]                                                   {'loss': 1.1424, 'grad_norm': 5.567782878875732, 'learning_rate': 1.2054794520547945e-06, 'num_tokens': 256463.0, 'mean_token_accuracy': 0.6937804892659187, 'epoch': 0.03}
  3%|▎         | 44/1449 [15:30<8:10:59, 20.97s/it]  3%|▎         | 45/1449 [15:52<8:12:10, 21.03s/it]                                                   {'loss': 1.263, 'grad_norm': 4.855529308319092, 'learning_rate': 1.232876712328767e-06, 'num_tokens': 263402.0, 'mean_token_accuracy': 0.6684509664773941, 'epoch': 0.03}
  3%|▎         | 45/1449 [15:52<8:12:10, 21.03s/it]  3%|▎         | 46/1449 [16:12<8:10:32, 20.98s/it]                                                   {'loss': 1.1312, 'grad_norm': 4.8933305740356445, 'learning_rate': 1.2602739726027396e-06, 'num_tokens': 269073.0, 'mean_token_accuracy': 0.7006328292191029, 'epoch': 0.03}
  3%|▎         | 46/1449 [16:12<8:10:32, 20.98s/it]  3%|▎         | 47/1449 [16:34<8:11:41, 21.04s/it]                                                   {'loss': 1.3152, 'grad_norm': 5.848649501800537, 'learning_rate': 1.2876712328767124e-06, 'num_tokens': 274713.0, 'mean_token_accuracy': 0.658977422863245, 'epoch': 0.03}
  3%|▎         | 47/1449 [16:34<8:11:41, 21.04s/it]  3%|▎         | 48/1449 [16:54<8:10:03, 20.99s/it]                                                   {'loss': 1.2144, 'grad_norm': 5.3240861892700195, 'learning_rate': 1.3150684931506847e-06, 'num_tokens': 280310.0, 'mean_token_accuracy': 0.6800337620079517, 'epoch': 0.03}
  3%|▎         | 48/1449 [16:54<8:10:03, 20.99s/it]  3%|▎         | 49/1449 [17:16<8:11:06, 21.05s/it]                                                   {'loss': 1.1891, 'grad_norm': 5.007350921630859, 'learning_rate': 1.3424657534246575e-06, 'num_tokens': 286192.0, 'mean_token_accuracy': 0.6851340048015118, 'epoch': 0.03}
  3%|▎         | 49/1449 [17:16<8:11:06, 21.05s/it]  3%|▎         | 50/1449 [17:37<8:09:23, 20.99s/it]                                                   {'loss': 1.0799, 'grad_norm': 4.840311527252197, 'learning_rate': 1.36986301369863e-06, 'num_tokens': 292452.0, 'mean_token_accuracy': 0.6927551031112671, 'epoch': 0.03}
  3%|▎         | 50/1449 [17:37<8:09:23, 20.99s/it][INFO|trainer.py:3966] 2025-06-06 00:27:03,035 >> Saving model checkpoint to /n/fs/similarity/open-r1/data/Qwen2.5-1.5B-Instruct-SFT-v4/checkpoint-50
[INFO|configuration_utils.py:423] 2025-06-06 00:27:03,040 >> Configuration saved in /n/fs/similarity/open-r1/data/Qwen2.5-1.5B-Instruct-SFT-v4/checkpoint-50/config.json
[INFO|configuration_utils.py:908] 2025-06-06 00:27:03,043 >> Configuration saved in /n/fs/similarity/open-r1/data/Qwen2.5-1.5B-Instruct-SFT-v4/checkpoint-50/generation_config.json
[INFO|modeling_utils.py:3586] 2025-06-06 00:27:11,464 >> Model weights saved in /n/fs/similarity/open-r1/data/Qwen2.5-1.5B-Instruct-SFT-v4/checkpoint-50/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-06-06 00:27:11,467 >> tokenizer config file saved in /n/fs/similarity/open-r1/data/Qwen2.5-1.5B-Instruct-SFT-v4/checkpoint-50/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-06-06 00:27:11,469 >> Special tokens file saved in /n/fs/similarity/open-r1/data/Qwen2.5-1.5B-Instruct-SFT-v4/checkpoint-50/special_tokens_map.json
[2025-06-06 00:27:11,693] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step50 is about to be saved!
[2025-06-06 00:27:11,700] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /n/fs/similarity/open-r1/data/Qwen2.5-1.5B-Instruct-SFT-v4/checkpoint-50/global_step50/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-06-06 00:27:11,700] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /n/fs/similarity/open-r1/data/Qwen2.5-1.5B-Instruct-SFT-v4/checkpoint-50/global_step50/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-06-06 00:27:11,715] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /n/fs/similarity/open-r1/data/Qwen2.5-1.5B-Instruct-SFT-v4/checkpoint-50/global_step50/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-06-06 00:27:11,716] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /n/fs/similarity/open-r1/data/Qwen2.5-1.5B-Instruct-SFT-v4/checkpoint-50/global_step50/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-06-06 00:27:34,958] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /n/fs/similarity/open-r1/data/Qwen2.5-1.5B-Instruct-SFT-v4/checkpoint-50/global_step50/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-06-06 00:27:35,004] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /n/fs/similarity/open-r1/data/Qwen2.5-1.5B-Instruct-SFT-v4/checkpoint-50/global_step50/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-06-06 00:27:35,035] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step50 is ready now!
[INFO|tokenization_utils_base.py:2510] 2025-06-06 00:27:46,529 >> tokenizer config file saved in /n/fs/similarity/open-r1/data/Qwen2.5-1.5B-Instruct-SFT-v4/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-06-06 00:27:46,531 >> Special tokens file saved in /n/fs/similarity/open-r1/data/Qwen2.5-1.5B-Instruct-SFT-v4/special_tokens_map.json
  4%|▎         | 51/1449 [18:44<13:31:22, 34.82s/it]                                                    {'loss': 1.094, 'grad_norm': 4.727295875549316, 'learning_rate': 1.3972602739726028e-06, 'num_tokens': 298737.0, 'mean_token_accuracy': 0.7104731500148773, 'epoch': 0.04}
  4%|▎         | 51/1449 [18:44<13:31:22, 34.82s/it]  4%|▎         | 52/1449 [19:06<12:01:31, 30.99s/it]                                                    {'loss': 1.3792, 'grad_norm': 4.5847625732421875, 'learning_rate': 1.424657534246575e-06, 'num_tokens': 304751.0, 'mean_token_accuracy': 0.6548600681126118, 'epoch': 0.04}
  4%|▎         | 52/1449 [19:06<12:01:31, 30.99s/it]  4%|▎         | 53/1449 [19:28<10:57:13, 28.25s/it]                                                    {'loss': 1.1681, 'grad_norm': 4.1287641525268555, 'learning_rate': 1.4520547945205479e-06, 'num_tokens': 311016.0, 'mean_token_accuracy': 0.6813653185963631, 'epoch': 0.04}
  4%|▎         | 53/1449 [19:28<10:57:13, 28.25s/it]  4%|▎         | 54/1449 [19:49<10:10:53, 26.28s/it]                                                    {'loss': 1.2079, 'grad_norm': 4.447936534881592, 'learning_rate': 1.4794520547945204e-06, 'num_tokens': 317207.0, 'mean_token_accuracy': 0.6782286278903484, 'epoch': 0.04}
  4%|▎         | 54/1449 [19:49<10:10:53, 26.28s/it]  4%|▍         | 55/1449 [20:11<9:40:59, 25.01s/it]                                                    {'loss': 1.289, 'grad_norm': 4.602755069732666, 'learning_rate': 1.5068493150684932e-06, 'num_tokens': 323095.0, 'mean_token_accuracy': 0.6698384955525398, 'epoch': 0.04}
  4%|▍         | 55/1449 [20:11<9:40:59, 25.01s/it]  4%|▍         | 56/1449 [20:33<9:18:12, 24.04s/it]                                                   {'loss': 1.1801, 'grad_norm': 4.4206461906433105, 'learning_rate': 1.5342465753424655e-06, 'num_tokens': 329037.0, 'mean_token_accuracy': 0.6925205439329147, 'epoch': 0.04}
  4%|▍         | 56/1449 [20:33<9:18:12, 24.04s/it]  4%|▍         | 57/1449 [20:55<9:01:46, 23.35s/it]                                                   {'loss': 1.1282, 'grad_norm': 4.293708801269531, 'learning_rate': 1.5616438356164383e-06, 'num_tokens': 335175.0, 'mean_token_accuracy': 0.6979105100035667, 'epoch': 0.04}
  4%|▍         | 57/1449 [20:55<9:01:46, 23.35s/it]  4%|▍         | 58/1449 [21:17<8:50:24, 22.88s/it]                                                   {'loss': 1.1724, 'grad_norm': 4.580864429473877, 'learning_rate': 1.5890410958904108e-06, 'num_tokens': 341195.0, 'mean_token_accuracy': 0.6909306794404984, 'epoch': 0.04}
  4%|▍         | 58/1449 [21:17<8:50:24, 22.88s/it]  4%|▍         | 59/1449 [21:38<8:38:04, 22.36s/it]                                                   {'loss': 1.2432, 'grad_norm': 4.255706310272217, 'learning_rate': 1.6164383561643836e-06, 'num_tokens': 347227.0, 'mean_token_accuracy': 0.6854199059307575, 'epoch': 0.04}
  4%|▍         | 59/1449 [21:38<8:38:04, 22.36s/it]  4%|▍         | 60/1449 [21:59<8:32:06, 22.12s/it]                                                   {'loss': 1.313, 'grad_norm': 4.926303386688232, 'learning_rate': 1.643835616438356e-06, 'num_tokens': 352785.0, 'mean_token_accuracy': 0.6490506902337074, 'epoch': 0.04}
  4%|▍         | 60/1449 [21:59<8:32:06, 22.12s/it]