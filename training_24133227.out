✅ Conda env active at: /n/fs/similarity/open-r1/openr1/bin/python
Python 3.11.12
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: write).
The token `ionic` has been saved to /n/fs/similarity/open-r1/.hf_cache/stored_tokens
Your token has been saved to /n/fs/similarity/open-r1/.hf_cache/token
Login successful.
Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.
✅ Logged into Hugging Face
Requirement already satisfied: yq in ./openr1/lib/python3.11/site-packages (3.4.3)
Requirement already satisfied: huggingface_hub in ./openr1/lib/python3.11/site-packages (0.32.4)
Requirement already satisfied: PyYAML>=5.3.1 in ./openr1/lib/python3.11/site-packages (from yq) (6.0.2)
Requirement already satisfied: xmltodict>=0.11.0 in ./openr1/lib/python3.11/site-packages (from yq) (0.14.2)
Requirement already satisfied: tomlkit>=0.11.6 in ./openr1/lib/python3.11/site-packages (from yq) (0.13.2)
Requirement already satisfied: argcomplete>=1.8.1 in ./openr1/lib/python3.11/site-packages (from yq) (3.6.2)
Requirement already satisfied: filelock in ./openr1/lib/python3.11/site-packages (from huggingface_hub) (3.18.0)
Requirement already satisfied: fsspec>=2023.5.0 in ./openr1/lib/python3.11/site-packages (from huggingface_hub) (2024.6.1)
Requirement already satisfied: packaging>=20.9 in ./openr1/lib/python3.11/site-packages (from huggingface_hub) (25.0)
Requirement already satisfied: requests in ./openr1/lib/python3.11/site-packages (from huggingface_hub) (2.32.3)
Requirement already satisfied: tqdm>=4.42.1 in ./openr1/lib/python3.11/site-packages (from huggingface_hub) (4.67.1)
Requirement already satisfied: typing-extensions>=3.7.4.3 in ./openr1/lib/python3.11/site-packages (from huggingface_hub) (4.12.2)
Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in ./openr1/lib/python3.11/site-packages (from huggingface_hub) (1.1.2)
Requirement already satisfied: charset-normalizer<4,>=2 in ./openr1/lib/python3.11/site-packages (from requests->huggingface_hub) (3.4.2)
Requirement already satisfied: idna<4,>=2.5 in ./openr1/lib/python3.11/site-packages (from requests->huggingface_hub) (3.10)
Requirement already satisfied: urllib3<3,>=1.21.1 in ./openr1/lib/python3.11/site-packages (from requests->huggingface_hub) (2.4.0)
Requirement already satisfied: certifi>=2017.4.17 in ./openr1/lib/python3.11/site-packages (from requests->huggingface_hub) (2025.4.26)
🟢 Setup complete. Ready to run SFT.
Env:        /n/fs/similarity/open-r1/openr1
Config:     recipes/Qwen2.5-1.5B-Instruct/sft/config_demo_liv.yaml
Log Files:  logs/liv_train_Qwen1.5B-SFT-Finetune-v2_20250606_000856.log
CUDA_VISIBLE_DEVICES: 0,1
/n/fs/similarity/open-r1/openr1/lib/python3.11/site-packages/transformers/utils/hub.py:105: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[2025-06-06 00:09:03,575] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
W0606 00:09:04.892000 1950090 torch/distributed/run.py:792] 
W0606 00:09:04.892000 1950090 torch/distributed/run.py:792] *****************************************
W0606 00:09:04.892000 1950090 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0606 00:09:04.892000 1950090 torch/distributed/run.py:792] *****************************************
/n/fs/similarity/open-r1/openr1/lib/python3.11/site-packages/transformers/utils/hub.py:105: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/n/fs/similarity/open-r1/openr1/lib/python3.11/site-packages/transformers/utils/hub.py:105: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[2025-06-06 00:09:10,243] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-06 00:09:10,244] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /n/fs/similarity/open-r1/.triton, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /n/fs/similarity/open-r1/.triton, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[2025-06-06 00:09:11,197] [INFO] [comm.py:652:init_distributed] cdb=None
[2025-06-06 00:09:11,211] [INFO] [comm.py:652:init_distributed] cdb=None
[2025-06-06 00:09:11,211] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
2025-06-06 00:09:11 - INFO - __main__ - Model parameters ModelConfig(model_name_or_path='Qwen/Qwen2.5-1.5B-Instruct', model_revision='main', torch_dtype='bfloat16', trust_remote_code=False, attn_implementation='flash_attention_2', use_peft=False, lora_r=16, lora_alpha=32, lora_dropout=0.05, lora_target_modules=None, lora_modules_to_save=None, lora_task_type='CAUSAL_LM', use_rslora=False, use_dora=False, load_in_8bit=False, load_in_4bit=False, bnb_4bit_quant_type='nf4', use_bnb_nested_quant=False)
2025-06-06 00:09:11 - INFO - __main__ - Script parameters ScriptArguments(dataset_name='open-r1/OpenR1-Math-220k', dataset_config='default', dataset_train_split='train', dataset_test_split='test', gradient_checkpointing_use_reentrant=False, ignore_bias_buffers=False, dataset_prompt_column='problem', dataset_response_column='solution', dataset_mixture=None)
2025-06-06 00:09:11 - INFO - __main__ - Training parameters SFTConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
benchmarks=[],
bf16=True,
bf16_full_eval=False,
callbacks=[],
chars_per_token=<CHARS_PER_TOKEN>,
chat_template=None,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
dataset_batch_size=None,
dataset_kwargs=None,
dataset_num_proc=None,
dataset_text_field=text,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=4,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_packing=None,
eval_steps=500000,
eval_strategy=IntervalStrategy.STEPS,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs={'use_reentrant': False},
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=Qwen2.5-1.5B-Instruct-SFT,
hub_model_revision=main,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-06,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=info,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/n/fs/similarity/open-r1/data/Qwen2.5-1.5B-Instruct-SFT-v4/runs/Jun06_00-09-11_node207.ionic.cs.princeton.edu,
logging_first_step=True,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_length=1024,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
model_init_kwargs=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_of_sequences=None,
num_train_epochs=1,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=/n/fs/similarity/open-r1/data/Qwen2.5-1.5B-Instruct-SFT-v4,
overwrite_hub_revision=False,
overwrite_output_dir=True,
packing=False,
padding_free=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=2,
prediction_loss_only=False,
push_to_hub=True,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_revision=False,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=Qwen1.5B-SFT-Finetune-v2-20250606_000856,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=50,
save_strategy=SaveStrategy.STEPS,
save_total_limit=100,
seed=42,
skip_memory_metrics=True,
split_batches=None,
system_prompt=You are a helpful AI Assistant that provides well-reasoned and detailed responses. You first think about the reasoning process as an internal monologue and then provide the user with the answer. Respond in the following format:
<think> ... </think> <answer> ... </answer>
,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tp_size=0,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger=None,
use_liger_kernel=False,
use_mps_device=False,
wandb_entity=None,
wandb_project=None,
wandb_run_group=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=0.0,
)
2025-06-06 00:09:11 - INFO - open_r1.utils.data - Loading dataset: open-r1/OpenR1-Math-220k/default
2025-06-06 00:09:11 - INFO - open_r1.utils.data - Loading dataset: open-r1/OpenR1-Math-220k/default
2025-06-06 00:09:13 - INFO - open_r1.utils.data - Tokenizing split 'train' (93733 examples)…
2025-06-06 00:09:13 - WARNING - __main__ - Requested split 'test' not found in ['train']. Creating a 1 % evaluation split from the training data.
[2025-06-06 00:09:13,556] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2
[WARNING|logging.py:329] 2025-06-06 00:09:13,558 >> You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Overwrite dataset info from restored data version if exists.
2025-06-06 00:09:13 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /n/fs/similarity/open-r1/.cache/huggingface/datasets/open-r1___open_r1-math-220k/default/0.0.0/e4e141ec9dea9f8326f4d347be56105859b2bd68
2025-06-06 00:09:13 - INFO - datasets.info - Loading Dataset info from /n/fs/similarity/open-r1/.cache/huggingface/datasets/open-r1___open_r1-math-220k/default/0.0.0/e4e141ec9dea9f8326f4d347be56105859b2bd68
Found cached dataset open_r1-math-220k (/n/fs/similarity/open-r1/.cache/huggingface/datasets/open-r1___open_r1-math-220k/default/0.0.0/e4e141ec9dea9f8326f4d347be56105859b2bd68)
2025-06-06 00:09:13 - INFO - datasets.builder - Found cached dataset open_r1-math-220k (/n/fs/similarity/open-r1/.cache/huggingface/datasets/open-r1___open_r1-math-220k/default/0.0.0/e4e141ec9dea9f8326f4d347be56105859b2bd68)
Loading Dataset info from /n/fs/similarity/open-r1/.cache/huggingface/datasets/open-r1___open_r1-math-220k/default/0.0.0/e4e141ec9dea9f8326f4d347be56105859b2bd68
2025-06-06 00:09:13 - INFO - datasets.info - Loading Dataset info from /n/fs/similarity/open-r1/.cache/huggingface/datasets/open-r1___open_r1-math-220k/default/0.0.0/e4e141ec9dea9f8326f4d347be56105859b2bd68
[INFO|tokenization_utils_base.py:2060] 2025-06-06 00:09:14,001 >> loading file vocab.json from cache at /n/fs/similarity/open-r1/.cache/huggingface/transformers/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/vocab.json
[INFO|tokenization_utils_base.py:2060] 2025-06-06 00:09:14,001 >> loading file merges.txt from cache at /n/fs/similarity/open-r1/.cache/huggingface/transformers/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/merges.txt
[INFO|tokenization_utils_base.py:2060] 2025-06-06 00:09:14,001 >> loading file tokenizer.json from cache at /n/fs/similarity/open-r1/.cache/huggingface/transformers/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/tokenizer.json
[INFO|tokenization_utils_base.py:2060] 2025-06-06 00:09:14,001 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2060] 2025-06-06 00:09:14,001 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2060] 2025-06-06 00:09:14,001 >> loading file tokenizer_config.json from cache at /n/fs/similarity/open-r1/.cache/huggingface/transformers/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/tokenizer_config.json
[INFO|tokenization_utils_base.py:2060] 2025-06-06 00:09:14,001 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2323] 2025-06-06 00:09:14,219 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2025-06-06 00:09:14 - INFO - open_r1.utils.data - Tokenizing split 'train' (93733 examples)…
Loading cached processed dataset at /n/fs/similarity/open-r1/.cache/huggingface/datasets/open-r1___open_r1-math-220k/default/0.0.0/e4e141ec9dea9f8326f4d347be56105859b2bd68/cache-6aeed7f1e1542bea.arrow
2025-06-06 00:09:14 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /n/fs/similarity/open-r1/.cache/huggingface/datasets/open-r1___open_r1-math-220k/default/0.0.0/e4e141ec9dea9f8326f4d347be56105859b2bd68/cache-6aeed7f1e1542bea.arrow
2025-06-06 00:09:14 - WARNING - __main__ - Requested split 'test' not found in ['train']. Creating a 1 % evaluation split from the training data.
Loading cached split indices for dataset at /n/fs/similarity/open-r1/.cache/huggingface/datasets/open-r1___open_r1-math-220k/default/0.0.0/e4e141ec9dea9f8326f4d347be56105859b2bd68/cache-f214169554aea0b8.arrow and /n/fs/similarity/open-r1/.cache/huggingface/datasets/open-r1___open_r1-math-220k/default/0.0.0/e4e141ec9dea9f8326f4d347be56105859b2bd68/cache-8ca86c422b54feb8.arrow
2025-06-06 00:09:14 - INFO - datasets.arrow_dataset - Loading cached split indices for dataset at /n/fs/similarity/open-r1/.cache/huggingface/datasets/open-r1___open_r1-math-220k/default/0.0.0/e4e141ec9dea9f8326f4d347be56105859b2bd68/cache-f214169554aea0b8.arrow and /n/fs/similarity/open-r1/.cache/huggingface/datasets/open-r1___open_r1-math-220k/default/0.0.0/e4e141ec9dea9f8326f4d347be56105859b2bd68/cache-8ca86c422b54feb8.arrow
[INFO|tokenization_utils_base.py:2060] 2025-06-06 00:09:14,335 >> loading file vocab.json from cache at /n/fs/similarity/open-r1/.cache/huggingface/transformers/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/vocab.json
[INFO|tokenization_utils_base.py:2060] 2025-06-06 00:09:14,335 >> loading file merges.txt from cache at /n/fs/similarity/open-r1/.cache/huggingface/transformers/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/merges.txt
[INFO|tokenization_utils_base.py:2060] 2025-06-06 00:09:14,335 >> loading file tokenizer.json from cache at /n/fs/similarity/open-r1/.cache/huggingface/transformers/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/tokenizer.json
[INFO|tokenization_utils_base.py:2060] 2025-06-06 00:09:14,335 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2060] 2025-06-06 00:09:14,335 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2060] 2025-06-06 00:09:14,335 >> loading file tokenizer_config.json from cache at /n/fs/similarity/open-r1/.cache/huggingface/transformers/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/tokenizer_config.json
[INFO|tokenization_utils_base.py:2060] 2025-06-06 00:09:14,335 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2323] 2025-06-06 00:09:14,543 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:699] 2025-06-06 00:09:14,588 >> loading configuration file config.json from cache at /n/fs/similarity/open-r1/.cache/huggingface/transformers/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json
[INFO|configuration_utils.py:771] 2025-06-06 00:09:14,590 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1536,
  "initializer_range": 0.02,
  "intermediate_size": 8960,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 12,
  "num_hidden_layers": 28,
  "num_key_value_heads": 2,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.50.0",
  "use_cache": false,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|modeling_utils.py:1154] 2025-06-06 00:09:14,638 >> loading weights file model.safetensors from cache at /n/fs/similarity/open-r1/.cache/huggingface/transformers/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/model.safetensors
[INFO|modeling_utils.py:2170] 2025-06-06 00:09:14,639 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.
[INFO|modeling_utils.py:3747] 2025-06-06 00:09:14,639 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[2025-06-06 00:09:14,639] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2
[WARNING|logging.py:329] 2025-06-06 00:09:14,642 >> You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[INFO|configuration_utils.py:1139] 2025-06-06 00:09:14,646 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "use_cache": false
}

[2025-06-06 00:09:15,767] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 339, num_elems = 1.78B
[INFO|modeling_utils.py:4987] 2025-06-06 00:09:17,094 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.

[INFO|modeling_utils.py:4995] 2025-06-06 00:09:17,094 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2.5-1.5B-Instruct.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1094] 2025-06-06 00:09:17,135 >> loading configuration file generation_config.json from cache at /n/fs/similarity/open-r1/.cache/huggingface/transformers/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/generation_config.json
[INFO|configuration_utils.py:1139] 2025-06-06 00:09:17,136 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.1,
  "temperature": 0.7,
  "top_k": 20,
  "top_p": 0.8
}

Loading cached processed dataset at /n/fs/similarity/open-r1/.cache/huggingface/datasets/open-r1___open_r1-math-220k/default/0.0.0/e4e141ec9dea9f8326f4d347be56105859b2bd68/cache-212db3323b9eb092.arrow
2025-06-06 00:09:17 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /n/fs/similarity/open-r1/.cache/huggingface/datasets/open-r1___open_r1-math-220k/default/0.0.0/e4e141ec9dea9f8326f4d347be56105859b2bd68/cache-212db3323b9eb092.arrow
Loading cached processed dataset at /n/fs/similarity/open-r1/.cache/huggingface/datasets/open-r1___open_r1-math-220k/default/0.0.0/e4e141ec9dea9f8326f4d347be56105859b2bd68/cache-eb0e30a8f560726f.arrow
2025-06-06 00:09:17 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /n/fs/similarity/open-r1/.cache/huggingface/datasets/open-r1___open_r1-math-220k/default/0.0.0/e4e141ec9dea9f8326f4d347be56105859b2bd68/cache-eb0e30a8f560726f.arrow
[INFO|trainer.py:748] 2025-06-06 00:09:17,197 >> Using auto half precision backend
2025-06-06 00:09:17 - INFO - __main__ - *** Train ***
[INFO|deepspeed.py:386] 2025-06-06 00:09:17,433 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
Using /n/fs/similarity/open-r1/.cache/torch_extensions/py311_cu124 as PyTorch extensions root...
Emitting ninja build file /n/fs/similarity/open-r1/.cache/torch_extensions/py311_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Using /n/fs/similarity/open-r1/.cache/torch_extensions/py311_cu124 as PyTorch extensions root...
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 0.2819085121154785 seconds
Loading extension module cpu_adam...
Time to load cpu_adam op: 0.3584756851196289 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000002, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
[2025-06-06 00:09:18,033] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.15.4, git-hash=unknown, git-branch=unknown
[2025-06-06 00:09:18,033] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2
[2025-06-06 00:09:18,043] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-06-06 00:09:18,044] [INFO] [logging.py:128:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2025-06-06 00:09:18,044] [INFO] [logging.py:128:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-06-06 00:09:18,059] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2025-06-06 00:09:18,059] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2025-06-06 00:09:18,059] [INFO] [logging.py:128:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
[2025-06-06 00:09:18,060] [INFO] [logging.py:128:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
[2025-06-06 00:09:18,241] [INFO] [utils.py:781:see_memory_usage] Stage 3 initialize beginning
[2025-06-06 00:09:18,242] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.87 GB         CA 0.0 GB         Max_CA 1 GB 
[2025-06-06 00:09:18,242] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 23.48 GB, percent = 4.7%
[2025-06-06 00:09:18,243] [INFO] [stage3.py:166:__init__] Reduce bucket size 500000000
[2025-06-06 00:09:18,243] [INFO] [stage3.py:167:__init__] Prefetch bucket size 50000000
[2025-06-06 00:09:18,389] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2025-06-06 00:09:18,390] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
[2025-06-06 00:09:18,390] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 23.48 GB, percent = 4.7%
Parameter Offload: Total persistent parameters: 144896 in 141 params
[2025-06-06 00:09:18,549] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2025-06-06 00:09:18,550] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
[2025-06-06 00:09:18,550] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 23.48 GB, percent = 4.7%
[2025-06-06 00:09:18,700] [INFO] [utils.py:781:see_memory_usage] Before creating fp16 partitions
[2025-06-06 00:09:18,700] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
[2025-06-06 00:09:18,700] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 23.48 GB, percent = 4.7%
[2025-06-06 00:09:19,937] [INFO] [utils.py:781:see_memory_usage] After creating fp16 partitions: 2
[2025-06-06 00:09:19,938] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
[2025-06-06 00:09:19,938] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 28.1 GB, percent = 5.6%
[2025-06-06 00:09:20,124] [INFO] [utils.py:781:see_memory_usage] Before creating fp32 partitions
[2025-06-06 00:09:20,125] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
[2025-06-06 00:09:20,125] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 28.87 GB, percent = 5.7%
[2025-06-06 00:09:21,243] [INFO] [utils.py:781:see_memory_usage] After creating fp32 partitions
[2025-06-06 00:09:21,244] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
[2025-06-06 00:09:21,244] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 33.96 GB, percent = 6.8%
[2025-06-06 00:09:21,425] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-06-06 00:09:21,425] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
[2025-06-06 00:09:21,425] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 34.41 GB, percent = 6.8%
[2025-06-06 00:09:22,634] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-06-06 00:09:22,634] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
[2025-06-06 00:09:22,634] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 39.8 GB, percent = 7.9%
[2025-06-06 00:09:22,635] [INFO] [stage3.py:521:_setup_for_real_optimizer] optimizer state initialized
[2025-06-06 00:09:23,553] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-06-06 00:09:23,553] [INFO] [utils.py:782:see_memory_usage] MA 0.93 GB         Max_MA 1.8 GB         CA 1.82 GB         Max_CA 2 GB 
[2025-06-06 00:09:23,553] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 42.55 GB, percent = 8.5%
[2025-06-06 00:09:23,554] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer_Stage3
[2025-06-06 00:09:23,554] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2025-06-06 00:09:23,554] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-06-06 00:09:23,554] [INFO] [logging.py:128:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.999), (0.9, 0.999)]
[2025-06-06 00:09:23,554] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   amp_enabled .................. False
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   amp_params ................... False
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f02bbfc4050>
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   communication_data_type ...... None
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   disable_allgather ............ False
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   dump_state ................... False
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   global_rank .................. 0
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 16
[2025-06-06 00:09:23,555] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   optimizer_name ............... None
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   optimizer_params ............. None
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   pld_enabled .................. False
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   pld_params ................... False
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   scheduler_name ............... None
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   scheduler_params ............. None
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   sparse_attention ............. None
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   steps_per_print .............. inf
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   train_batch_size ............. 64
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  2
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   world_size ................... 2
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  True
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   zero_enabled ................. True
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[2025-06-06 00:09:23,556] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3
[2025-06-06 00:09:23,556] [INFO] [config.py:989:print_user_config]   json = {
    "train_batch_size": 64, 
    "train_micro_batch_size_per_gpu": 2, 
    "gradient_accumulation_steps": 16, 
    "zero_optimization": {
        "stage": 3, 
        "offload_optimizer": {
            "device": "cpu", 
            "nvme_path": null
        }, 
        "offload_param": {
            "device": "cpu", 
            "nvme_path": null
        }, 
        "stage3_gather_16bit_weights_on_model_save": true
    }, 
    "gradient_clipping": 1.0, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
[INFO|trainer.py:2409] 2025-06-06 00:09:23,558 >> ***** Running training *****
[INFO|trainer.py:2410] 2025-06-06 00:09:23,558 >>   Num examples = 92,795
[INFO|trainer.py:2411] 2025-06-06 00:09:23,558 >>   Num Epochs = 1
[INFO|trainer.py:2412] 2025-06-06 00:09:23,558 >>   Instantaneous batch size per device = 2
[INFO|trainer.py:2415] 2025-06-06 00:09:23,558 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:2416] 2025-06-06 00:09:23,558 >>   Gradient Accumulation steps = 16
[INFO|trainer.py:2417] 2025-06-06 00:09:23,558 >>   Total optimization steps = 1,449
[INFO|trainer.py:2418] 2025-06-06 00:09:23,559 >>   Number of trainable parameters = 1,543,714,304
[INFO|integration_utils.py:831] 2025-06-06 00:09:23,560 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: Currently logged in as: ogd3 (ogd3-princeton-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /n/fs/similarity/open-r1/.wandb/wandb/run-20250606_000923-v96u0y67
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Qwen1.5B-SFT-Finetune-v2-20250606_000856
wandb: ⭐️ View project at https://wandb.ai/ogd3-princeton-university/huggingface
wandb: 🚀 View run at https://wandb.ai/ogd3-princeton-university/huggingface/runs/v96u0y67
  0%|          | 0/1449 [00:00<?, ?it/s]  0%|          | 1/1449 [00:25<10:17:21, 25.58s/it]                                                   {'loss': 1.4013, 'grad_norm': 7.115165710449219, 'learning_rate': 2.73972602739726e-08, 'num_tokens': 5427.0, 'mean_token_accuracy': 0.6334721110761166, 'epoch': 0.0}
  0%|          | 1/1449 [00:25<10:17:21, 25.58s/it]  0%|          | 2/1449 [00:46<9:17:44, 23.13s/it]                                                   {'loss': 1.2827, 'grad_norm': 6.971931457519531, 'learning_rate': 5.47945205479452e-08, 'num_tokens': 11489.0, 'mean_token_accuracy': 0.6730691567063332, 'epoch': 0.0}
  0%|          | 2/1449 [00:46<9:17:44, 23.13s/it]  0%|          | 3/1449 [01:08<8:54:19, 22.17s/it]                                                  {'loss': 1.193, 'grad_norm': 6.016913414001465, 'learning_rate': 8.21917808219178e-08, 'num_tokens': 18093.0, 'mean_token_accuracy': 0.683920793235302, 'epoch': 0.0}
  0%|          | 3/1449 [01:08<8:54:19, 22.17s/it]  0%|          | 4/1449 [01:29<8:44:12, 21.77s/it]                                                  {'loss': 1.3166, 'grad_norm': 6.0954179763793945, 'learning_rate': 1.095890410958904e-07, 'num_tokens': 25298.0, 'mean_token_accuracy': 0.6579648293554783, 'epoch': 0.0}
  0%|          | 4/1449 [01:29<8:44:12, 21.77s/it]  0%|          | 5/1449 [01:49<8:35:32, 21.42s/it]                                                  {'loss': 1.2731, 'grad_norm': 7.529391288757324, 'learning_rate': 1.36986301369863e-07, 'num_tokens': 30708.0, 'mean_token_accuracy': 0.6679505221545696, 'epoch': 0.0}
  0%|          | 5/1449 [01:49<8:35:32, 21.42s/it]  0%|          | 6/1449 [02:11<8:33:21, 21.35s/it]                                                  {'loss': 1.3951, 'grad_norm': 7.666150093078613, 'learning_rate': 1.643835616438356e-07, 'num_tokens': 36237.0, 'mean_token_accuracy': 0.6484273560345173, 'epoch': 0.0}
  0%|          | 6/1449 [02:11<8:33:21, 21.35s/it]  0%|          | 7/1449 [02:32<8:32:03, 21.31s/it]                                                  {'loss': 1.3743, 'grad_norm': 6.7357258796691895, 'learning_rate': 1.917808219178082e-07, 'num_tokens': 42724.0, 'mean_token_accuracy': 0.635077141225338, 'epoch': 0.0}
  0%|          | 7/1449 [02:32<8:32:03, 21.31s/it]  1%|          | 8/1449 [02:53<8:28:15, 21.16s/it]                                                  {'loss': 1.3128, 'grad_norm': 7.435177326202393, 'learning_rate': 2.191780821917808e-07, 'num_tokens': 48140.0, 'mean_token_accuracy': 0.6607662849128246, 'epoch': 0.01}
  1%|          | 8/1449 [02:53<8:28:15, 21.16s/it]