# ─── Model arguments ─────────────────────────────────────────────────────────
model_name_or_path:       Qwen/Qwen2.5-1.5B-Instruct
model_revision:           main
torch_dtype:              bfloat16
attn_implementation:      flash_attention_2

# ─── Data training arguments ─────────────────────────────────────────────────
dataset_name:             open-r1/OpenR1-Math-220k
dataset_config:           default
dataset_prompt_column:    problem
dataset_response_column:  solution
dataset_train_split:      train
dataset_test_split:       train   # Math-220k has only "train"; eval on same

system_prompt: >
  You are a helpful AI Assistant that provides well-reasoned and detailed responses.
  You first think about the reasoning process as an internal monologue and then provide
  the user with the answer. Respond in the following format:

  <think>
  ...
  </think>
  <answer>
  ...
  </answer>

# ─── GRPO trainer config ─────────────────────────────────────────────────────
beta:                         0.01
bf16:                         true
use_vllm:                     true
do_eval:                      false
gradient_accumulation_steps:  16
gradient_checkpointing:       true
gradient_checkpointing_kwargs:
  use_reentrant:              false
hub_model_id:                 Qwen2.5-1.5B-Open-R1-Math-GRPO
hub_strategy:                 every_save
learning_rate:                5.0e-06
log_completions:              true
log_level:                    info
logging_first_step:           true
logging_steps:                1
logging_strategy:             steps
lr_scheduler_type:            cosine_with_min_lr
lr_scheduler_kwargs:
  min_lr_rate:                0.1
max_prompt_len:               1024
max_completion_len:           1024
max_steps:                    2000
num_generations:              14           # 7 GPUs × 16 batch = 112 global; 112 ÷ 14 = 8
num_train_epochs:             1
output_dir:                   data/Qwen2.5-1.5B-Open-R1-Math-GRPO
overwrite_output_dir:         true
per_device_train_batch_size:  2
push_to_hub:                  true
report_to:
  - wandb
reward_funcs:
  - accuracy
  - format
reward_weights:
  - 1.0
  - 0.1
save_strategy:                steps
save_steps:                   25
save_total_limit:             1
seed:                         42
temperature:                  1.0
warmup_ratio:                 0.03
target_kl: 0.02
kl_horizon: 64
adapt_kl_coef: true
init_kl_coef: 0.2